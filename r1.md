# Summary


CMS plans major detector upgrades for the HL-LHC era, so that the detectors can cope with
radiation effects and increased instantaneous luminosity. At the HL-LHC,
trigger rates are expected to be ten times higher than in Run 3,
and event data will be much more complex
due to larger particle multiplicities
and overlaid events from pileup of 140-200 and more.

Analyzing HL-LHC data will require massively increased computing
and data handling capabilities. While the computing technical design report
is not expected before 2020, this document gives initial estimates of the
expected needs, for the rest of Run 2 and Run 3 up to the first years of the HL-LHC,
as seen from the U.S.CMS perspective.

All estimates show that mere extrapolation of current computing approaches and
traditional resources will likely result in large computing resource shortfalls.
Between the last year of Run 3 (2023) and the startup year of HL-LHC (2026)
processing resource needs are estimated to increase by a factor of 30,
the total disk storage needed will be almost 3,000 Petabyte, requiring a 15-fold increase,
and the long-term storage capacity for disaster recovery (tape) will be beyond 1,700 Petabyte.

With the current technology outlook and flat computing budgets, we expect that CMS will 
suffer from a large resource gap to meet the hugely increased computing needs of the HL-LHC era. 
Additional resources from HPC centers and from commercial clouds could contribute to
close this gap, if they can be brought to bear.

CMS already has made progress in adapting the CMS software and application framework
to use HPC centers and commercial clouds, as described in this note. 
Simulation and reconstruction workflows have been run successfully at HPC centers,
and are partially integrated into the automated workflow and data management systems.
However, much additional work is still needed.

Looking into the future, our studies indicate that up to 100\% of 
U.S.-based **production** workflows (simulation, reconstruction, etc)
could be run on HPC centers if these resources became available.
To reach that goal, a number of important issues regarding 
efficiency and throughput, data access, automation, security, etc
will need to be addressed and require additional
engineering effort. 
For global CMS, at the HL-LHC, production will take up about 60\% of the
total processing resources,
and U.S. CMS would be expected to contribute 40% of these.

For **data analysis**, CMS will require in addition
almost he same amount of computing capacity,
if we extrapolate from today's needs.
Compared to production workflows, analysis is dominated by iterative
and fast access to data with quite different I/O characteristics from production.
Currently there is no good match for these analysis workflows to
the kind of services and architectures that HPC centers offer.
However, CMS and U.S. CMS are embarking on R\&D to make analysis computing more
effective. Dedicated analysis facilities will play an important role in addressing
future HL-LHC analysis needs.
Storage and computational
resources will need to be much more tightly coupled and optimized to address data analysis needs.
We should investigate if and how HPC centers could play a role in this evolved analysis paradigm.
That would  require additional engineering and close collaboration with HPC providers.

The time scales involved are long compared to the rate of evolution of computing technology,
and we expect significant and possibly disruptive technology advances that could
bring needs and capabilities closer together in the future, if U.S. CMS can
develop, grow, and maintain
sufficient engineering capabilities. 

CMS increasingly depends on capabilities beyond the traditional batch processing,
in particular elasticity in resource provisioning to address peak demands,
short- and long-term resource planning, and
inclusion of allocation-based resources. 
These bring new challenges with incorporating cloud and HPC resources into the planning,
robust accounting and monitoring, and extending automation to non-owned resources. 
There are also issues with access and security requirements that continue to be challenging. 

However, those are technical and political challenges that can be solved in pursuit of the goal
of moving most of the production resources towards external providers and HPC, 
at estimated capacity needs of 23 THS06s  per year for Run 3 (equivalent to 0.64 Billion core hours per year on current Xeon processors),
and 800 THS06s per year for the initial HL-LHC (equivalent to 22 Billion core hours per year on current Xeon processors). 

U.S. CMS welcomes the initiative of the HEP office to work with ASCR to develop
long-term plans for ASCR to support the LHC data intensive and high-throughput
computing needs as part of their facility. 

