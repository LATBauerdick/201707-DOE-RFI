Workflows
-------------

CMS executes a variety of tasks on its distributed computing infrastructure, for reconstructing collision data recorded by the detector, simulating collision data, and analyzing both. In the following, we call all information of a collision, both for data from the detector and for simulations, an event. CMS uses an object-oriented event data model to persist information of an event in a file based on ROOT. Standardized groups of objects are persisted in different files, for example all objects that are related to the raw detector information. Files with the same event content are described by a common "data tier" identifier. CMS distinguishes six main types of computing tasks.

The **gridpack integration** workflow is a preparation step for simulating, or generating, events from theoretical principles. It performs the phase space integration for specific physics processes before the actual events are generated to save processing time later. It is executed by CMS physicists and not by central production teams as it needs close supervision and normally requires several iterations to guarantee the correctness of the physics processes. In the following, we do not show the resource needs of the gridpack integration separately and assume that it is included in the analysis CPU resources.  The output is small, of the order of tens of gigabyte per gridpack, and is provided to generation applications through CVMFS. The CPU needs for this step are negligible compared to the other steps.  

The **generation** task uses software packages provided by the theoretical particle physics community to generate events from basic theoretical principles. The events are stored in text files in a standardized format called the LesHouches Event accord (LHE) and consist only of a few numbers per event. If possible, these generators use the aforementioned gridpacks to avoid repetition of the phase space integration. In most cases, the generation task is executed by central production teams as part of the full simulation workflow (generation + simulation + digitization + reconstruction, see later). In some special cases, the generation is not run centrally. In this case, CMS physicists execute the generation step like in the gridpack integration step to guarantee correctness of the generated physics processes. These special cases are also included in the analysis resources. CMS is currently not using generators that benefit from massive MPI execution (Sherpa, Alpgen). CMS’s primary generator is aMC+Madgraph and CMS’ use of generators today is a negligible part of its total CPU needs given the generators it uses. We note that the CPU time per event during generation differs by more than an order of magnitude between different generators simulating identical physics. There is a fundamental problem with next-to-leading order (NLO) generators in that their use of negative event weights leads to anywhere between $\times 3$ to $\times 25$ the number of events required for leading order simulations to achieve the same statistical precision. In all our projections for CPU needs we assume that the problem of negative weights in NLO generators will be solved by the phenomenology community, which develops these generators. If it is not solved, the CPU needs for simulation from generation through reconstruction will increase by an order of magnitude.

The **simulation** task is part of the full simulation workflow and uses Geant4 to simulate the energy deposition of the generated particles in the CMS detector components. It is centrally executed. The time to simulate an event for LHC Run 2 beam conditions is roughly the same as for the HL-LHC — the additional complexity comes through the Pile-up simulation, which is done in a later step. The output of the simulation task is called the GEN-SIM data tier.

The **digitization** task simulates detector component signals from the energy depositions determined during the simulation task . The digitization task also simulates the LHC beam conditions, characterized by the number of additional collisions happening when the two proton bunches collide in the detector, called pileup. CMS distinguishes two pileup simulation modes. The standard pileup mixing combines a number of individually simulated minimum bias events as a secondary input to the digitization task along with the primary simulated event.  In LHC Run 2, about a hundred minimum bias events are needed to digitize one generated event. This increases to the order of a thousand for HL-LHC. Reading hundreds or thousands of simulated minimum bias events to digitize a primary event causes significant I/O load for the applications, making streaming impossible, especially in the HL-LHC case. To lower the I/O load, CMS developed pre-mixing of pileup, where the combination of minimum bias events is done in a separate step. These pre-mixed events can be re-used in the simulation of different generated samples. The digitization step in pre-mixing mode combines a simulated primary event with a pre-mixed secondary event. The ability to pre-mix pileup requires an excellent understanding of the detector component behaviors under LHC running conditions. The pre-mixing mode of pileup in LHC Run 2 consumes $\frac{1}{2}$ of the CPU time of standard pileup per event. For the HL-LHC, digitization dominates the CPU budget per event for the sequence of generation, simulation, and digitization. At present, CMS is unable to use pre-mixing for HL-LHC simulations. The projections thus assume standard pileup mixing.  

The **reconstruction** task combines all algorithms to reconstruct detector component signals as well as global event quantities. It is the same task both for simulated and recorded events.  The output of the reconstruction task is optimized for the analysis task that follows. Apart from the RECO data tier that is used only in rare cases for detector component commissioning, the main output data tiers are AOD (Analysis Object Data) and MINIAOD (a smaller subset of the AOD). The reconstruction time per event increases non-linearly with number of pileup events per collision. This explains the large difference in CPU time per event between LHC Run 2 and HL-LHC. To reduce this increase significantly would require changes in the detector geometry and/or giving up on low pT tracking. The latter is expected to compromise the physics performance provided by particle flow algorithms.  

The **analysis** task reduces the output of the reconstruction through slimming and skimming. Analysts also calculate specific event properties that are then combined with the centrally provided information to produce plots and tables. Analysts can also write out ntuples at the end or during the reduction process; the corresponding data tier is called USER. The total disk budget of CMS worldwide is dominated by the needs of analyzers to easily and reliably access the reconstructed data and to be able to process it quickly and efficiently. The present model of needs for HL-LHC assumes analysis can be done using MINIAOD. If instead, a significant fraction of the analyses require AOD, then this estimate of needs is  too optimistic. If, on the other hand, a format can be developed that is significantly smaller than the MINIAOD then the needs would shrink.  CMS is starting R&D in 2017 to define such a smaller format, currently referred to as MICROAOD. The CPU budget for analysis is driven by the average number of events per second analyzed. At present, analysis on MINIAOD achieves an average of 5 Hz. If a more refined data format could be developed that requires less detailed computations from the end user then the analysis CPU needs might decrease significantly.  

These tasks are combined into three primary workflow types, although all tasks could be executed individually or in different combinations. The full simulation workflow consists of the generation + simulation + digitization + reconstruction tasks and produces AOD and MINIAOD output. The reconstruction workflow uses the RAW data tier as input and produces AOD and MINIAOD output. The analysis workflow is the most diverse workflow and is not handled centrally, using primarily MINIAOD as input, and can produce USER ntuples apart from plots and tables. CMS assumes that all intermediate outputs of the workflows remain on the compute resource, and has the ability to choose which outputs of a given workflow are archived to storage.  
