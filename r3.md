Status of Using HPC and Commercial Clouds
-------------------------------------------------

### Executing CMS Applications on Intel Knights Landing Architectures

We have successfully demonstrated the multi-threaded execution of CMS’
physics software on Intel Knights Landing (KNL) CPU architectures, by
running the reconstruction algorithms on input data and producing
analysis output data.  Comparing event throughput per thread, the KNL-based
host has 5x less throughput than a 2010-era Xeon host.  A cost extrapolation
suggests that, dollar-for-dollar, the KNL platform is far less efficient than a
traditional Intel Xeon host.

CMS’ physics application demonstrates near-linear scaling up to 64
threads; we fully utilize a current KNL host by running 4 multi-threaded
physics applications in parallel. The factor limiting the number of
threads is the need to serialize writing of results to storage, using
the third-party ROOT libraries.  Moving to a parallel-friendly I/O
system would allow CMS to fully utilize the system with just one job
instance. 


To improve overall throughput on KNL hosts requires major refactoring
of CMS software, to make better use of vectorization. This mean a
significant overhaul of millions of lines of code.  Currently the \"hot
spots\" account for less than 5% of the total processing time, suggesting
a broad end-to-end redesign of reconstruction algorithms would be needed
to better leverage vectorization and increase memory locality.
 As such a code overhaul would also benefit the traditional Xeon
platforms, we believe the performance gap would decrease but not
close.

### HPC Centers

U.S. CMS has demonstrated production-use of HPC centers with an
emphasis on the main central CMS workflows, starting with the
re-reconstruction of LHC Run 1 data at SDSC, to the the recent
production use of allocations at NERSC for full Monte Carlo production
workflows. Access to HPC resources is provided through HEPCloud at
Fermilab for work submitted to the CMS global glideinWMS pool. 


In the last months we made significant progress in our ability to use
NERSC HPC resources for full simulation workflows, at the relatively low
level of running about 5,000 jobs in parallel. CPU efficiency is
reasonably high on standard Xeon architectures and the workflows behave
similar to running on regular Grid sites. We are using pre-mixed pileup
workflows that stream the pileup events from Fermilab. We have been able
to run at much higher scales, achieving this for short amounts of time,
but are starting to run into overload situations of the shared NERSC
infrastructure. 

This situation would worsen if we were forced to use standard pileup
mixing to minimize network data access to Fermilab, and a solution would
have to be found. We have a very good relationship to NERSC staff and
are working with them to remove these limitations. A bottleneck we are
frequently encountering is ramp up time, limiting the \"elasticity\" of
resource provisioning for overflowing to HPC resources. It takes
significant time to have glideinWMS pilots start at NERSC and provide
significant resources compared to standard grid sites. This challenge
will have to be addressed to enable substantial use of these
resources.

### Commercial Clouds

U.S. CMS has demonstrated that we can execute large-scale simulation
workflows [^1]
generation + simulation + digitization + reconstruction) on
the Amazon Elastic Compute Cloud (EC2) and Google Cloud Platform
(GCP), via the Fermilab HEPCloud pilot project. Workloads were
sustained at 60,000 concurrent cores on EC2 (increasing CMS global
resource use by 25%) and 160,000 concurrent cores on GCP (increase of
\~100%). An important factor in the success of these studies was that
ESnet established high-speed (100 Gb/s) peering points with the
commercial cloud providers.

[^1]: Given that these platforms charge an egress
fee per GB that leaves their network, these workflows were chosen
specifically to optimize cost, maximizing CPU use while minimizing the
size of the output data.

The cost per utilized core-hour was estimated to currently be 60-80%
higher than the cost of executing these workflows at the Fermilab Tier-1
facility; we anticipate cloud cost to decrease while industry continues
to invest \$30B/quarter into their infrastructure and service offerings.
 As in the HPC case, HEPCloud at Fermilab is used to provide access
to commercial cloud resources for work submitted to the CMS
global glideinWMS pool. The HEPCloud program is scheduled to
transition to production/operations by the end of 2018.

