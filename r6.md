Resource Needs in the U.S.
----------------------------------

The previous sections gave detailed estimates for resource need
profiles for CMS, covering the years leading up to the HL-LHC and the
initial running years of high-luminosity operations. To estimate the
computing resource cost for the U.S. we assume, based on the U.S.
commitment for Run 1 and Run 2 to provide 40% of CMS Tier-1 resources,
that for HL-LHC the U.S. will continue to provide 40% of the CMS
production computing resource needs.


In addition, the U.S. CMS program has to provide computing resources to
support data analysis for U.S. scientists. Ten years of historic data
show that data analysis resource needs closely track production
computing resource needs. U.S. scientists have consistently used about
50% of the total available computational and data storage resources in
U.S. CMS, across the Tier-1, Tier-2, and LPC-CAF analysis facilities.
Our resource plans for U.S. CMS thus include computational and disk
storage resources for analysis at about equal size to the
corresponding production resources the U.S. is providing. This estimate
is consistent with the modelled analysis needs as shown above
(considering that &gt;30% of CMS analysts are U.S. scientists, and
recognizing the leadership role of the U.S. in physics analysis and use
of data analysis resources).

### U.S. CMS CPU needs

+-----------------------+-----------------------+-----------------------+
| [Year]{.c0}           | [CPU time (THS06      | [Fraction that could  |
|                       | s)]{.c0}              | be run on HPC or      |
|                       |                       | commercial            |
|                       |                       | clouds]{.c0}          |
+-----------------------+-----------------------+-----------------------+
| [2017]{.c0}           | [15.1]{.c0}           | [0.50]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2018]{.c0}           | [15.3]{.c0}           | [0.50]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2019]{.c0}           | [26.6]{.c0}           | [0.67]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2020]{.c0}           | [15.8]{.c0}           | [0.45]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2021]{.c0}           | [17.7]{.c0}           | [0.45]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2022]{.c0}           | [19.9]{.c0}           | [0.40]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2023]{.c0}           | [22.1]{.c0}           | [0.36]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2024]{.c0}           | [32.3]{.c0}           | [0.53]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2025]{.c0}           | [161]{.c0}            | [0.57]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2026]{.c0}           | [467]{.c0}            | [0.44]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2027]{.c0}           | [634]{.c0}            | [0.44]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2028]{.c0}           | [579]{.c0}            | [0.44]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2029]{.c0}           | [531]{.c0}            | [0.44]{.c0}           |
+-----------------------+-----------------------+-----------------------+
| [2030]{.c0}           | [488]{.c0}            | [0.44]{.c0}           |
+-----------------------+-----------------------+-----------------------+

[Table 3:]{.c2}[ U.S. CMS CPU requirements estimate. The integrated CPU
time needed each year is given in Tera-HEPspec06 seconds (1 THS06s
roughly corresponds to 100 Billion \"Service Units\" or CPU seconds on a
Intel Xeon-type CPU), together with the fraction of CPU time that could
be provided through commercial cloud services or through allocations at
HPC centers, (with the provisos given in the text). ]{.c0}

Table 3 gives the estimate of U.S. CMS CPU needs through 2030.  In
addition, we estimate what fraction of these resources can be provided
by HPC centers.  We assume that all workflows except for the prompt
reconstruction of detector data and user analysis can be performed at
the centers, but not necessarily at the same event throughput per dollar
as described above.  The challenges of using HPC centers for these
workflows is described in detail below.

As alternative scenarios, we consider possible improvements to the CPU
needs due to HL-LHC software improvements.  At the moment, HL-LHC
digitization is done through standard pileup mixing.  If pre-mixing can
be implemented for the HL-LHC simulation, the digitization time could be
reduced by 50%, leading to an approximately 10% reduction in overall CPU
needs.  Alternatively we can consider more aggressive improvements in
the reconstruction algorithms that could lead to 20%
decreases in CPU time.  This would lead to an approximately 15%
reduction in overall CPU needs.  These two steps together could thus
reduce CPU needs by 25%, as they have independent impacts.  Such
aggressive improvements would require significant investments in
software engineering.  To this end, the U.S. CMS Operations Program
intends to add more effort in this area as we approach the HL-LHC
era.

Many steps can be considered to reduce the requirements for analysis
computing and to make HPC centers more functional for production
computing, as we describe in the following sections.

### U.S. Analysis CPU Needs

During LHC Run 1 and Run 2 about half of the experiment’s computing
resources have gone into data analysis and processing performed by
individuals and small working groups, as opposed to the *production
workflows* run by the central computing operation teams.

For HL-LHC we expect the analysis resource need to continue to
scale in step with the production resource needs, and possibly more
steeply. CMS does not yet have a bottom-up model to assess future
analysis needs. Given the increased scales involved it is clear that
data analysis at HL-LHC appears to become a huge problem without an
obvious solution.


To continue to support hundreds of U.S. scientists in data analysis,
optimizations of the current approach are needed to reign in the vast
multiplication of data transformations and subsequent intermediate
storage of a large number of differing data products for individual
further analysis. Analysis requires on-demand execution, i.e.
reasonably fast turnaround. Programs are individually
written, adjusted and modified, less debugged than production workflows,
and not tuned for efficient execution. Analysis activity tends to spike
before major conferences.


New analysis paradigms and strategies will be required to address the
analysis needs of the HL-LHC. We propose to invest R&D effort in
dedicated analysis facilities that could streamline some of the
individual work- and data flows. Optimizing data analysis would also
bring about an evolution towards interactive \"data mining\" access to
centralized data repositories hosted at the large data centers, with
massive computational capabilities for latency-optimized fast-turnaround
data transformations.


Improving turnaround times and latencies in performing data analysis,
even as the data sample sizes increase massively, would significantly
improve scientific productivity minimizing the \"time to insight\"
for CMS analyses. For data analysis, capacity of storage space,
performance of data access, and in particular I/O performance and
latency, become the key concern. This is different from the production
processing needs for elastic capacity and optimizing for overall
throughput, where latency of individual processes are less of a concern.


Data analysis typically involves plowing through ever-increasing event
samples to map and reduce data down to histograms and to perform
statistical analysis. Orders of magnitude decreases in
latency and event-throughput are needed to address this issue. This
will require a more centralized end-to-end engineered approach to
data analysis facilities to improve on the current capabilities and
to enable analysis of much larger datasets than today.

R&D will be needed into data formats, compression algorithms, and new
ways of storing and accessing data for analysis, to investigate
optimizing the storage systems and data representation on disk
together, and also to facilitate the utilization of new additional
storage layers like SSD storage and NVRAM-like storage that exhibit
different characteristics than the currently dominant spinning disk
installations. These developments should include a fresh look at the
concept of \"virtual data\", considering re-computing quantities instead
of storing pre-computed values. Issues to be studied are relative cost
and performance in optimizing resource use and performance parameters
like throughput and latency of response. We propose prototyping such
analysis facilities to develop the capabilities and the required
ecosystem to support the analysis use cases.  ]{.c0}

It is of yet unclear what role HPC installations can play in this area,
if any. In the current model these HPC facilities are available to our
community mainly through providing allocation-based resources, instead
of highly \"elastic\" on-demand, low latency response resource pools.
 Data analysis seems to be most challenging for the current HPC
facilities model, and may be best addressed by dedicated analysis
facilities with tight CPU to storage connection. Towards HL-LHC we
envision dedicated data analysis facilities for experimenters that
provide an extendable environment that provide fully functional analysis
capabilities. In such an environment HPC facilities could play a central
role as \"data transformation engines,\" if a host of technology, data
access and security issues can be resolved.


### U.S. CMS Production CPU needs

Table 3 shows the fraction of production workflows that could be
executed on HPC machines and commercial cloud resources. The main
workflows to consider are data re-reconstruction and full simulation.
Our model assumes that on the HL-LHC time scale the relevant HPC machine
architecture will be based on Intel KNL CPUs or their successors. We
note that it is quite likely that not all national scale HPC resources
will be deployed using processor architectures that our applications can
be ported to with reasonable amounts of effort. As a result, it is quite
likely that we will be able to use only a subset of the available
national scale HPC resources. Executing our production workflows
on HPC machines and commercial clouds providers poses specific
challenges that we would need to overcome to effectively make use of
these resources.

The first challenge in using HPC resources is the distributed nature of
the CMS collaboration and how the collaboration’s software is developed.
Hundreds of developers from over 40 countries contribute to the
development of the CMS software that is used to execute the main
production workflows. This is in conflict with the user model
that HPC installations are used to, in which only a few collaborators
are responsible for the software that is executed. We need to
make sure that the requirements of the HPC centers continue to be
compatible with the software development process of a large
HEP collaboration.


The allocation-based resource assignment on HPC centers poses its own
challenges to CMS. CMS is accustomed to receiving constant yearly
allocations of computing resources and has the ability to use
temporarily unused resources opportunistically. In contrast, HPC centers
assign allocations to users which can be used in a specific time frame.
The absence of a guaranteed resource assignment is a challenge
for CMS to accommodate. On the other hand, HPC centers with
significantly more resources than the CMS allocation offer the
possibility of elastic use of allocations in shorter time frames to
handle peaks in resource needs. We would like to work with the HPC
centers to handle allocation-based resources for HL-LHC computing needs
that best meet the operational needs of CMS.


The CMS software framework is currently used in multi-threaded mode in
production with 4-8 threads. At 8 threads, we start to observe
inefficiencies due to the current output I/O handling. These
inefficiencies become more severe at higher thread counts, as observed
when running on KNL architectures. We have begun R&D this year to
address this bottleneck and will need continued support to
effectively use KNL architectures in the HL-LHC era.

The product of event throughput rate and memory required is the
appropriate benchmark to measure the effective use of a CPU
resource. A multi-threaded application will not have the same event
throughput as the corresponding number of single-threaded applications
but will use significantly less total memory. On modern high-core CPU
architectures, it is of paramount importance to run in
multi-threaded mode because of the limited memory per core. In addition,
the individual cores in KNL architectures are optimized for vectorized
code. As stated above, the CMS algorithms are currently not optimized
for vectorization, thus the event throughput on KNL is 5 times smaller
than on Xeon architectures at comparable thread counts. R&D will be
needed to vectorize reconstruction algorithms and engineering effort
will be needed to implement the algorithms within the optimized
framework.

HPC centers are designed for large applications spanning many
individual nodes. CMS’ application is most efficient single threaded and
only to optimize memory consumption and to exploit new architectures is
it using multi-threading and vectorization. The CMS application
will be able to reach modestly high thread counts but will never be
efficient at extremely high number of cores. Therefore the CMS
application will not reasonably be able to use more cores than on a
single node on an HPC machine, and might even need to run multiple
applications depending on the characteristics of the nodes. To
efficiently make use of allocations at HPC centers, CMS will need to be
able to start sufficient numbers of applications and sustain these
numbers to achieve good throughput. Current experience shows that this
is not always easy and R&D will be needed to optimize this. The ramp-up
time is currently a limiting factor as well. Improvements would allow
CMS to exploit elasticity if resources are available. Because experiment
software and alignment and calibration constants need to be finalized
and certified before production starts, shortening the production time
significantly would mean that improvements in software and conditions
could be integrated into physics analyses much quicker. Therefore being
able to exploit elasticity would increase the physics output.

This would better match CMS’s operations mode. Final software
versions and calibration and alignment constants need to be available
well in advance to allow for the production of the samples. If the
production time can be shortened, this would increase physics output.

Data handling at the scales required for LHC and HL-LHC science will be
challenging for HPC centers. In all cases, sufficient network bandwidth
in and out of the HPC center will be needed to either use local disk for
caching or streaming access to data. All of CMS’s workflows need
to write significant amounts of output either locally or
directly off-site. Further R&D and integration work will be needed to
identify and remove bottlenecks.

The full simulation workflow does not need arge amounts of input
data like the data reconstruction workflow does, but does need
access to minimum bias samples for pileup simulation. For pre-mixed
pileup access, streaming could be feasible, but at very high core counts
this would need significant network bandwidth to serve pileup to
the applications. A single pre-mixed pileup minimum bias sample for
the simulation of one LHC Run 2 running period requires
about 500 TB of disk space and could be stored locally if disk space is
provided.  However, standard mixing will be used in HL-LHC simulations
for some time to come, requiring the input of the order of 1000
minimum bias events in addition to the primary interaction.
Streaming in this scenario becomes close to impossible. The minimum bias
sample for standard mixing in Run 2 has an average size of
50-100 TB. Providing low latency and high throughput access to this data
at the HPC center would be needed to run the full simulation workflow
efficiently. Reading many small events requires lower latency than
reading one larger pre-mixed event to keep the efficiency high, even if
it is significantly larger in size.


Commercial clouds are currently 1.5 times more expensive to provision
and operate than HEP owned resources, based on using the most
cost effective cloud resources available, including the spot price
market of AWS. But commercial cloud providers are ideal for elastic
scale out as demonstrated in the CMS pilot runs on EC2 and GCP.


Data access from the applications running in commercial clouds can be
provided through streaming as well as local access. Providers charge for
local disk space, and local data needs to be replicated to different
geographical regions to reach sufficient scale. While the transfer of
data into the clouds is free, sufficient network peering with the
scientific networks needs to be provided. Local space would be needed
for workflows where streaming input is not an option, such as the full
simulation workflow in standard mixing mode.


Commercial clouds charge for data egress. CPU dominated workflows like
data re-reconstruction and full simulation have low output volume to cpu
time fractions. This keeps the costs down and even qualifies for egress
waivers in certain cases.

### U.S. CMS Disk Needs

+-----------------------+-----------------------+-----------------------+
| [Year]{.c0}           | [Low \[PB\]]{.c0}     | [High \[PB\]]{.c0}    |
+-----------------------+-----------------------+-----------------------+
| [2017]{.c0}           | [5]{}[6]{.c0}         | [56]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2018]{.c0}           | [63]{.c0}             | [64]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2019]{.c0}           | [63]{.c0}             | [64]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2020]{.c0}           | [60]{.c0}             | [61]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2021]{.c0}           | [65]{.c0}             | [67]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2022]{.c0}           | [72]{.c0}             | [74]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2023]{.c0}           | [76]{.c0}             | [78]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2024]{.c0}           | [78]{.c0}             | [80]{.c0}             |
+-----------------------+-----------------------+-----------------------+
| [2025]{.c0}           | [160]{.c0}            | [178]{.c0}            |
+-----------------------+-----------------------+-----------------------+
| [2026]{.c0}           | [995]{.c0}            | [1,150]{.c0}          |
+-----------------------+-----------------------+-----------------------+
| [2027]{.c0}           | [1,614]{.c0}          | [1,870]{.c0}          |
+-----------------------+-----------------------+-----------------------+
| [2028]{.c0}           | [1,795]{.c0}          | [2,079]{.c0}          |
+-----------------------+-----------------------+-----------------------+
| [2029]{.c0}           | [1,864]{.c0}          | [2,152]{.c0}          |
+-----------------------+-----------------------+-----------------------+
| [2030]{.c0}           | [1,933]{.c0}          | [2,225]{.c0}          |
+-----------------------+-----------------------+-----------------------+

[Table 4: U.S. CMS disk requirements estimates, in units of
petabytes.]{.c0}

Table 4 shows the U.S. CMS disk requirements, which reach into the
exabyte range in the HL-LHC era.  The *High* scenario represents our
baseline model.  The *Low* scenario assumes that the AOD and MINIAOD
event sizes can be reduced by 20%, which leads to an approximately 15%
reduction in the overall disk needs.  It is hard to see how such disk
storage needs for the HL-LHC could be accommodated.  Transformative
changes will be required.


The U.S. CMS disk needs are dominated by the need to provide access to
data for analysis, which is traditionally very hard to model and
predict. 


The current analysis access model has worked well for LHC Run 1 and 2.
The model is based on efficient and performant local access at the Grid
sites along with the ability to stream data from the same resources to
all other Grid sites. The latter requires strong network capabilities.
The disk needs are determined by how many replicas are needed for
efficient global access as well as how many different versions (from
different software versions and constant sets to different file
contents) as well as the main analysis file content and its size. The
extrapolation of the current model based on MINIAOD as the main event
format will require significant disk resources for HL-LHC. As outlined
above, the HL-LHC analysis model needs to undergo significant changes to
be sustainable and performant for the masses of data and simulation
events. One aspect could be the development of a new, much smaller
primary analysis event format, called MICROAOD. It would not only reduce
the amount of disk needed to provide access to data and simulations for
analysis, it would also allow changes to the analysis flow and reduce
the USER space needed for ntuples. Preliminary estimates show a total
reduction of needed disk space for HL-LHC by a factor of approximately 5
are possible. To achieve this goal, effort would be needed to improve
the production infrastructure to output this new format much more
quickly and efficiently than before, as well as effort to facilitate the
usage of this new format using new analysis facility concepts as
outlined above.

### U.S. CMS tape needs

+-----------------------------------+-----------------------------------+
| [Year]{.c0}                       | [Nominal \[PB\]]{.c0}             |
+-----------------------------------+-----------------------------------+
| [2017]{.c0}                       | [65]{.c0}                         |
+-----------------------------------+-----------------------------------+
| [2018]{.c0}                       | [76]{.c0}                         |
+-----------------------------------+-----------------------------------+
| [2019]{.c0}                       | [80]{.c0}                         |
+-----------------------------------+-----------------------------------+
| [2020]{.c0}                       | [83]{.c0}                         |
+-----------------------------------+-----------------------------------+
| [2021]{.c0}                       | [95]{.c0}                         |
+-----------------------------------+-----------------------------------+
| [2022]{.c0}                       | [106]{.c0}                        |
+-----------------------------------+-----------------------------------+
| [2023]{.c0}                       | [118]{.c0}                        |
+-----------------------------------+-----------------------------------+
| [2024]{.c0}                       | [122]{.c0}                        |
+-----------------------------------+-----------------------------------+
| [2025]{.c0}                       | [172]{.c0}                        |
+-----------------------------------+-----------------------------------+
| [2026]{.c0}                       | [687]{.c0}                        |
+-----------------------------------+-----------------------------------+
| [2027]{.c0}                       | [1,299]{.c0}                      |
+-----------------------------------+-----------------------------------+
| [2028]{.c0}                       | [1,823]{.c0}                      |
+-----------------------------------+-----------------------------------+
| [2029]{.c0}                       | [2,341]{.c0}                      |
+-----------------------------------+-----------------------------------+
| [2030]{.c0}                       | [2,858]{.c0}                      |
+-----------------------------------+-----------------------------------+

[Table 5: U.S. CMS tape requirements estimates, in units of
petabytes.]{.c0}

Table 5 presents the U.S. CMS tape requirements. CMS will need disaster
recovery for its RAW data. The amount of tape space needed for RAW data
archiving is determined by the accelerator performance and the
trigger rate and is irreducible; it reaches the exabyte scale in the
HL-LHC era. We assume that tape will continue to be the technology of
choice for disaster recovery in the HL-LHC era. RAW data archiving
is the dominant element of tape usage as shown, therefore we do not
provide an alternative scenario. We plan to investigate cold storage
technologies including tape to further optimize disk needs for data that
is used rarely. The idea is to remove rarely used data from disk and
store it on cold storage, and retrieve it when it is needed while
retiring other rarely used data, therefore reducing the overall disk
needs. The presented tape estimate includes a model to use tape as cold
storage for AOD and MINIAOD samples.

`