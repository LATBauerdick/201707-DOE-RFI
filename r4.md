Estimate of LHC and CMS performance
-------------------------------------------

The overall scale of CMS’ required computing resources is driven by the total number of events, both simulated and recorded.  For our estimates, we follow the current long-range LHC schedule, which indicates shutdowns in 2019-20 and 2024-25, with otherwise full operational years, except in 2026 when HL-LHC will be commissioned.  We include a Hübner factor of 0.6 at a typical 150 data taking days per year to determine the number of running seconds each year (7.8 million).  

The CMS trigger rate for physics is currently 1 kHz, which we assume to remain constant until the HL-LHC era, when it is expected to increase to 10 kHz.A significant amount of computing resources go into Monte Carlo simulations.  We assume that twice as many events will be simulated as recorded in any given running year.  

We expect to reconstruct a given LHC year’s data and to generate, simulate and reconstruct the same year’s Monte Carlo samples within the same year.  The CPU times required per event are given in Table 1, in units of HS06 seconds, for both the LHC (pre-2026) and HL-LHC (2026 onwards) eras. We note that the fact that HS06 seconds per event for the simulation workflow is the same as the reconstruction workflow is pure coincidence. Moreover, each simulated event must also be reconstructed. The CPU cost per event for simulation is thus twice that for RAW data. As we expect to simulate twice as many events per year than we take in data, this implies that the total CPU cost of data processing is ¼ that of simulation processing per year.

CMS has historically made continual improvements in the execution time of simulation and reconstruction through improved algorithms and software engineering. These are included in the model, assuming steady but moderate year-by-year improvements for the LHC software in the near term, and then more aggressive improvements in the HL-LHC software starting in 2024, as the collaboration becomes more focused on HL-LHC preparations.

\pagebreak

  Workflow                              LHC events           HL-LHC events
  ------------------------ ----------------------- -----------------------
  Generation/simulation/   600                     4000
  digitization [HS06*s]
  Reconstruction [HS06*s]  250                     4000

  Table 1: Current processing times per event for different workflows in HS06 seconds.

The estimated sizes of each data tier for LHC and HL-LHC events are given in Table 2.

  Data Tier         LHC events        HL-LHC events     Use Case
  ----------------- ----------------- ----------------- -----------------
  RAW               1 MB              5 MB              LHC raw data
  GENSIM            1 MB              5 MB              Simulated events
  AOD               400 kB            2 MB              Input for 10% of analyses
  MINIAOD           40 kB             200 kB            Input for 90% of analyses
  USER              4 KB              10 kB             Unique per analysis

  Table 2: Sizes of each data tier for LHC and HL-LHC events.


The annual CPU requirements are primarily driven by the product of the number of events that must be processed and the processing time per event. Several additional factors are included to make the model more realistic. Some additional resources are needed for ancillary activities, such as alignment and calibration and are included with the prompt reconstruction of detector data in the category of "Prompt Data" below. Resources are allocated for both re-reconstructing detector data and re-simulating Monte Carlo events in response to improved understanding of detector calibrations and improved algorithms. These include "legacy" reconstruction a year’s samples at the end of each year, and samples for the entire run in the first year of a shutdown.

Resources for data analysis are estimated at a level that is consistent with current experience in the experiment, with increases in future years as LHC data accumulates.  In the pre-HL-LHC era we expect to simulate and reconstruct some number of HL-LHC events for detector and physics studies.  For the first few years this is projected as being just a few percent of the number of events that will be required during HL-LHC running, with increases as we get closer to the HL-LHC.  

The total disk and tape space needs are driven by the number of events recorded and simulated.  We take into account the fact that multiple disk replicas are needed of all the data in use for analysis in the distributed computing system.  In general, we assume that more disk replicas are kept of data recorded in recent years than of data from earlier years.  The detailed assumptions chosen in the model are based on our Run 2 experience.  The baseline assumption is that the MINIAOD tier will continue to be used for the vast majority of analyses.  

